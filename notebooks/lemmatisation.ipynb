{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Svetlana-Yatsyk/JdE_RMBLF_texto/blob/main/notebooks/lemmatisation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir le résultat, il vous suffit d’exécuter successivement **toutes les cellules** de ce notebook.<br><br>\n",
    "Pour cela, cliquez sur le **bouton «lecture»** à gauche de chaque cellule, ou utilisez le raccourci **Maj + Entrée**.<br><br>\n",
    "Il est très important de **respecter l’ordre d’exécution**, en allant de la première cellule jusqu’à la dernière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la première cellule, nous installons la bibliothèque **`stanza`**. Une bibliothèque est un ensemble de fonctions ou de commandes prêtes à l’emploi.<br><br>\n",
    "**`Stanza`** est une bibliothèque capable d’effectuer la **lemmatisation** et l’**analyse morpho-syntaxique**, c’est-à-dire l’identification des parties du discours. Elle est [adaptée à de nombreuses langues](https://stanfordnlp.github.io/stanza/performance.html) et, pour le latin, elle propose quatre modèles différents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /usr/local/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.11/site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/site-packages (from stanza) (4.25.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/site-packages (from stanza) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sy/Library/Python/3.11/lib/python/site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->stanza) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->stanza) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->stanza) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons maintenant la bibliothèque **`stanza`** et deux autres bibliothèques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant créer une variable contenant quelques versets du deuxième chapitre du Cantique des Cantiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "canticum = \"Sicut lilium inter spinas, sic amica mea inter filias. \" \\\n",
    "\"Sicut malus inter ligna silvarum, sic dilectus meus inter filios. \" \\\n",
    "\"Sub umbra illius quem desideraveram sedi, et fructus ejus dulcis gutturi meo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule crée un **pipeline stanza**, c’est-à-dire une chaîne de traitement. <br>\n",
    "Ici, on indique que la langue est le latin (`lang='la'`) et que l’on utilise le **modèle «perseus»**.<br><br>\n",
    "Le paramètre `processors` précise les étapes du traitement :\n",
    "\n",
    "* `tokenize` : découper le texte en mots (*tokens*),\n",
    "* `pos` : identifier la partie du discours (nom, verbe, adjectif, etc.),\n",
    "* `lemma` : ramener chaque mot à sa forme de base (lemmatisation),\n",
    "* `depparse` : analyser les relations syntaxiques entre les mots.\n",
    "\n",
    "Cela peut prendre un certain temps, car les modèles sont assez volumineux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 23:53:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 14.7MB/s]                    \n",
      "2025-10-26 23:53:18 INFO: Downloaded file to /Users/sy/stanza_resources/resources.json\n",
      "2025-10-26 23:53:18 WARNING: Language la package perseus expects mwt, which has been added\n",
      "2025-10-26 23:53:20 INFO: Loading these models for language: la (Latin):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | perseus          |\n",
      "| mwt       | perseus          |\n",
      "| pos       | perseus_nocharlm |\n",
      "| lemma     | perseus_nocharlm |\n",
      "| depparse  | perseus_nocharlm |\n",
      "================================\n",
      "\n",
      "2025-10-26 23:53:20 INFO: Using device: cpu\n",
      "2025-10-26 23:53:20 INFO: Loading: tokenize\n",
      "2025-10-26 23:53:22 INFO: Loading: mwt\n",
      "2025-10-26 23:53:22 INFO: Loading: pos\n",
      "2025-10-26 23:53:25 INFO: Loading: lemma\n",
      "2025-10-26 23:53:25 INFO: Loading: depparse\n",
      "2025-10-26 23:53:26 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='la', package=\"perseus\", processors='tokenize, pos, lemma, depparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande `nlp_stanza(canticum)` lance le traitement : `Stanza` segmente le texte, identifie les catégories grammaticales, attribue à chaque mot sa lemme, puis établit les relations syntaxiques entre eux.<br>\n",
    "Le résultat est enregistré dans une nouvelle variable appelée `canticum_lemmatised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "canticum_lemmatised=nlp_stanza(canticum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule parcourt le texte que nous avons analysé avec `stanza` et affiche, pour chaque mot, trois informations :\n",
    "\n",
    "* **`token.text`** → la forme originale du mot telle qu’elle apparaît dans le texte,\n",
    "* **`token.lemma`** → sa **lemme**, c’est-à-dire la forme de base du mot,\n",
    "* **`token.pos`** → sa **catégorie grammaticale** (nom, verbe, adjectif, etc.).\n",
    "\n",
    "Autrement dit, cette boucle permet de visualiser le résultat de la lemmatisation et de l’analyse morpho-syntaxique pour chaque mot du passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sicut - sicut - SCONJ\n",
      "lilium - lilius - NOUN\n",
      "inter - inter - ADP\n",
      "spinas - spina - NOUN\n",
      ", - , - PUNCT\n",
      "sic - sic - ADV\n",
      "amica - amica - NOUN\n",
      "mea - meus - DET\n",
      "inter - inter - ADP\n",
      "filias - filia - NOUN\n",
      ". - . - PUNCT\n",
      "Sicut - sicut - SCONJ\n",
      "malus - malus - ADJ\n",
      "inter - inter - ADP\n",
      "ligna - lignum - NOUN\n",
      "silvarum - silva - NOUN\n",
      ", - , - PUNCT\n",
      "sic - sic - ADV\n",
      "dilectus - diligo - VERB\n",
      "meus - meus - DET\n",
      "inter - inter - ADP\n",
      "filios - filius - NOUN\n",
      ". - . - PUNCT\n",
      "Sub - sub - ADP\n",
      "umbra - umbra - NOUN\n",
      "illius - ille - DET\n",
      "quem - qui - PRON\n",
      "desideraveram - desidero - VERB\n",
      "sedi - sedes - NOUN\n",
      ", - , - PUNCT\n",
      "et - et - CCONJ\n",
      "fructus - fructus - NOUN\n",
      "ejus - is - PRON\n",
      "dulcis - dulcis - ADJ\n",
      "gutturi - gunfero - VERB\n",
      "meo - meus - DET\n",
      ". - . - PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sent in canticum_lemmatised.sentences:\n",
    "  for token in sent.words:\n",
    "    print(token.text + ' - ' + token.lemma + ' - ' + token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de la Vulgate (Genesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule télécharge deux fichiers depuis le dépôt GitHub :\n",
    "\n",
    "1. **`stopwords_lat.txt`** — une liste de *mots vides* (mots très fréquents comme *et, in, de,* etc.),\n",
    "   que l’on exclut souvent des analyses statistiques, car ils n’apportent pas de sens lexical propre. Cette liste est créée par [Aurélien Berra](https://github.com/aurelberra/stopwords/tree/master) ;\n",
    "2. **`vulgata_full_clean.txt`** — le texte complet de la Vulgate nettoyé de ses références de chapitres et de versets.\n",
    "\n",
    "Ensuite :\n",
    "\n",
    "* la variable **`stopwords`** contient tous les mots vides sous forme de liste (un mot par ligne) ;\n",
    "* la variable **`vulgata`** contient tout le texte de la Vulgate sous forme d’une grande chaîne de caractères.\n",
    "\n",
    "Autrement dit, cette cellule prépare les **données textuelles** et la **liste de mots à ignorer** pour les étapes suivantes de l’analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-27 00:04:15--  https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/stopwords_lat.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31320 (31K) [text/plain]\n",
      "Saving to: ‘stopwords_lat.txt’\n",
      "\n",
      "stopwords_lat.txt   100%[===================>]  30,59K  --.-KB/s    in 0,01s   \n",
      "\n",
      "2025-10-27 00:04:15 (2,20 MB/s) - ‘stopwords_lat.txt’ saved [31320/31320]\n",
      "\n",
      "--2025-10-27 00:04:16--  https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/vulgata_full_clean.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4014585 (3,8M) [text/plain]\n",
      "Saving to: ‘vulgata_full_clean.txt’\n",
      "\n",
      "vulgata_full_clean. 100%[===================>]   3,83M  5,88MB/s    in 0,7s    \n",
      "\n",
      "2025-10-27 00:04:18 (5,88 MB/s) - ‘vulgata_full_clean.txt’ saved [4014585/4014585]\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/stopwords_lat.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/stopwords_lat.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/vulgata_full_clean.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/stopwords_lat.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m vulgata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/vulgata_full_clean.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/stopwords_lat.txt'"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/stopwords_lat.txt\n",
    "#!wget https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/vulgata_full_clean.txt\n",
    "!wget https://raw.githubusercontent.com/Svetlana-Yatsyk/JdE_RMBLF_texto/main/data/Genesis.txt\n",
    "\n",
    "stopwords = open(\"/content/stopwords_lat.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")\n",
    "#vulgata = open(\"/content/vulgata_full_clean.txt\",'r',encoding=\"utf8\").read()\n",
    "genesis = open(\"/content/Genesis.txt\",'r',encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 00:03:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 22.7MB/s]                    \n",
      "2025-10-27 00:03:22 INFO: Downloaded file to /Users/sy/stanza_resources/resources.json\n",
      "2025-10-27 00:03:22 WARNING: Language la package default expects mwt, which has been added\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.10.0/models/tokenize/ittb.pt: 100%|██████████| 624k/624k [00:00<00:00, 4.49MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.10.0/models/mwt/ittb.pt: 100%|██████████| 458k/458k [00:00<00:00, 6.09MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.10.0/models/pos/ittb_nocharlm.pt: 100%|██████████| 23.9M/23.9M [00:04<00:00, 5.77MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.10.0/models/lemma/ittb_nocharlm.pt: 100%|██████████| 3.21M/3.21M [00:00<00:00, 4.00MB/s]\n",
      "2025-10-27 00:03:30 INFO: Loading these models for language: la (Latin):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | ittb          |\n",
      "| mwt       | ittb          |\n",
      "| pos       | ittb_nocharlm |\n",
      "| lemma     | ittb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-10-27 00:03:30 INFO: Using device: cpu\n",
      "2025-10-27 00:03:30 INFO: Loading: tokenize\n",
      "2025-10-27 00:03:30 INFO: Loading: mwt\n",
      "2025-10-27 00:03:30 INFO: Loading: pos\n",
      "2025-10-27 00:03:35 INFO: Loading: lemma\n",
      "2025-10-27 00:03:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='la', processors='tokenize, pos, lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule définit une fonction qui traite le texte par lots («batches») à l’aide du modèle stanza, \n",
    "effectue la tokenisation, la lemmatisation et l’analyse morpho-syntaxique,\n",
    "puis enregistre les résultats dans un fichier CSV contenant pour chaque mot sa forme, sa lemme et sa catégorie grammaticale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(text, nlp, batch_size=100, output_prefix=\"genesis\"):\n",
    "    paragraphs = [p for p in text.split('\\n') if p.strip()]\n",
    "    words = []\n",
    "\n",
    "    for i in range(0, len(paragraphs), batch_size):\n",
    "        batch_text = '\\n'.join(paragraphs[i:i + batch_size])\n",
    "        doc = nlp(batch_text)\n",
    "        words.extend(\n",
    "            {\n",
    "                \"word\": w.text,\n",
    "                \"lemma\": w.lemma,\n",
    "                \"pos\": w.pos\n",
    "            }\n",
    "            for s in doc.sentences\n",
    "            for w in s.words\n",
    "            if w.lemma\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(words)\n",
    "    outdir = Path(\".\")\n",
    "    outdir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = outdir / f\"{output_prefix}_lemmatized.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Create TXTs\n",
    "    (outdir / f\"{output_prefix}_lemmas.txt\").write_text(\n",
    "        \"\\n\".join(df[\"lemma\"].tolist()), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    (outdir / f\"{output_prefix}_types.txt\").write_text(\n",
    "        \"\\n\".join(sorted(set(df[\"lemma\"]))), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    (outdir / f\"{output_prefix}_tokens.txt\").write_text(\n",
    "        \"\\n\".join(df[\"word\"].tolist()), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    print(\" Fichiers enregistrés :\")\n",
    "    print(f\" - {csv_path}\")\n",
    "    print(f\" - {output_prefix}_lemmas.txt\")\n",
    "    print(f\" - {output_prefix}_types.txt\")\n",
    "    print(f\" - {output_prefix}_tokens.txt\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance le traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genesis_lem = batch_process(genesis, nlp_stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule parcourt la liste des tokens issus de la lemmatisation. Pour chaque mot, elle extrait sa **forme originale** (`form`) et sa **lemme** (`lemma`).\n",
    "Si la lemme n’est pas un signe de ponctuation, elle est ajoutée aux listes `forms` et `lemmas`.\n",
    "Enfin, si la lemme n’est ni une ponctuation ni un mot vide (*stopword*), elle est ajoutée à la liste `no_stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = []\n",
    "lemmas = []\n",
    "no_stop = []\n",
    "\n",
    "for token in genesis_lem:\n",
    "    form = token[\"word\"]\n",
    "    lemma = token[\"lemma\"]\n",
    "\n",
    "    if lemma not in string.punctuation:\n",
    "        forms.append(form)\n",
    "        lemmas.append(lemma)\n",
    "\n",
    "    if lemma not in string.punctuation and lemma not in stopwords:\n",
    "        no_stop.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Nombre total de formes :\", len(forms))\n",
    "print(\"Nombre total de lemmes :\", len(lemmas))\n",
    "print(\"Nombre de lemmes sans stopwords :\", len(no_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code permet de générer les **nuage de mots**.\n",
    "<br><br><br>\n",
    "Je remercie **Marianne Reboul** pour ce code, ainsi que pour l’idée générale de ce notebook, qui reprend en grande partie l’un de ses [supports pédagogiques](https://colab.research.google.com/github/OdysseusPolymetis/journees_cluster5b_7/blob/main/3_nlp_lat_gk.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_word_cloud(words_list, title):\n",
    "    text = ' '.join(words_list)\n",
    "\n",
    "    radius = 495\n",
    "\n",
    "    diameter = radius * 2\n",
    "    center = radius\n",
    "    x, y = np.ogrid[:diameter, :diameter]\n",
    "    mask = (x - center) ** 2 + (y - center) ** 2 > radius ** 2\n",
    "    mask = 255 * mask.astype(int)\n",
    "\n",
    "    mask_rgba = np.dstack((mask, mask, mask, 255 - mask))\n",
    "\n",
    "    wordcloud = WordCloud(repeat=False, width=diameter, height=diameter,\n",
    "                          background_color=None, mode=\"RGBA\", colormap='plasma',\n",
    "                          mask=mask_rgba).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_cloud(forms, 'Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_cloud(lemma, 'Lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_cloud(no_stop, 'Lemmas sans mots vides')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule permet de télécharger les formes et les lemmes obtenus.<br>\n",
    "Ils pourront ensuite être importés dans Voyant Tools pour une analyse complémentaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"/content/forms.txt\", \"w\", encoding=\"utf8\") as f, open(\"/content/lemmas.txt\", \"w\", encoding=\"utf8\") as f2, open(\"/content/pullito.txt\", \"w\", encoding=\"utf8\") as f3:\n",
    "    f.write(\"\\n\".join(forms))\n",
    "    f2.write(\"\\n\".join(lemmas))\n",
    "    f3.write(\"\\n\".join(no_stop))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
